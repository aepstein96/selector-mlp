#!/bin/bash
# Script to run the full gene selector pipeline, including data splitting and model training

# Set default values
INPUT_DATA="raw_data/adata_variable_small.h5ad"
SPLIT_DIR="intermediate_files/split_datasets"
USE_SMALL_SPLIT=false
RESUME_TRAINING=false
RUN_DATA_SPLIT=true
MAX_EPOCHS=50

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    key="$1"
    case $key in
        --input)
            INPUT_DATA="$2"
            shift 2
            ;;
        --split_dir)
            SPLIT_DIR="$2"
            shift 2
            ;;
        --small_split)
            USE_SMALL_SPLIT=true
            shift
            ;;
        --resume)
            RESUME_TRAINING=true
            shift
            ;;
        --no_split)
            RUN_DATA_SPLIT=false
            shift
            ;;
        --epochs)
            MAX_EPOCHS="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Convert bash boolean values to Python boolean values
if [ "$RESUME_TRAINING" = true ]; then
    PY_RESUME_TRAINING="True"
else
    PY_RESUME_TRAINING="False"
fi

# Set dataset file paths based on the split size
if [ "$USE_SMALL_SPLIT" = true ]; then
    TRAIN_FILE="${SPLIT_DIR}/small_train.h5ad"
    VAL_FILE="${SPLIT_DIR}/small_val.h5ad"
    TEST_FILE="${SPLIT_DIR}/small_test.h5ad"
else
    TRAIN_FILE="${SPLIT_DIR}/standard_train.h5ad"
    VAL_FILE="${SPLIT_DIR}/standard_val.h5ad"
    TEST_FILE="${SPLIT_DIR}/standard_test.h5ad"
fi

# Create directories if they don't exist
mkdir -p raw_data
mkdir -p intermediate_files
mkdir -p ${SPLIT_DIR}
mkdir -p logs
mkdir -p results
mkdir -p results/figures
mkdir -p SVMs
mkdir -p feature_selection

# Step 1: Run data splitting if enabled
if [ "$RUN_DATA_SPLIT" = true ]; then
    echo "Step 1: Splitting dataset into train/validation/test sets..."
    python src/split_dataset.py --input "${INPUT_DATA}" --output_dir "${SPLIT_DIR}" --overwrite
    if [ $? -ne 0 ]; then
        echo "Error: Data splitting failed."
        exit 1
    fi
    echo "Data splitting completed successfully."
else
    echo "Step 1: Skipping data splitting as requested."
fi

# Step 2: Create config file with correct settings
CONFIG_FILE="src/temp_config.py"
cat > "${CONFIG_FILE}" << EOL
"""
Temporary configuration file generated by run_pipeline.sh
"""

CONFIG = {
    # Data settings
    'data_dir': 'raw_data',
    'data_file': '$(basename ${INPUT_DATA})',
    'target_column': 'Main_cluster_name',
    'sparse': False,
    
    # Pre-split data settings
    'use_presplit_data': True,
    'train_file': '${TRAIN_FILE}',
    'val_file': '${VAL_FILE}',
    'test_file': '${TEST_FILE}',
    
    # Training/validation/test split settings
    'max_train_cells': 5000,
    'max_val_cells': 1000,
    'max_test_cells': 1000,
    'split_seed': 42,
    
    # Training settings
    'batch_size': 256,
    'num_workers': 4,
    'learning_rate': 0.0003,
    'max_epochs': ${MAX_EPOCHS},
    'weight_decay': 0.3,
    'alpha': 0.05,
    'resume_training': ${PY_RESUME_TRAINING},
    
    # Model architecture settings (unchanged)
    'L1_size': 512,
    'L2_size': 128,
    'L3_size': 64,
    'leak_angle': 0.2,
    'dropout': 0.3,
    
    # MLP training conditions (unchanged)
    'mlp_conditions': [
        {'name': 'MLP_final', 'dropout': 0.3, 'batch_norm': True, 'noise': 0.2},
        {'name': 'MLP_no_dropout', 'dropout': 0, 'batch_norm': True, 'noise': 0.2},
        {'name': 'MLP_no_batchnorm', 'dropout': 0.3, 'batch_norm': False, 'noise': 0.2},
        {'name': 'MLP_no_noise', 'dropout': 0.3, 'batch_norm': True, 'noise': 0},
        {'name': 'MLP_nothing', 'dropout': 0, 'batch_norm': False, 'noise': 0}
    ],
    
    # Selector training conditions (unchanged)
    'selector_conditions': [
        {'name': 'None', 'reg_type': None},
        {'name': 'L1', 'reg_type': 'L1'},
        {'name': 'L2', 'reg_type': 'L2'},
        {'name': 'L1_L2', 'reg_type': 'L1/L2'},
    ],
    
    # Feature selection settings (unchanged)
    'num_reduced_features': 100,
    'feature_selection_group_size': 50,
    'feature_selection_random_reps': 5,
    
    # Directory settings (unchanged)
    'logs_dir': 'logs',
    'results_dir': 'results',
    
    # Pipeline control flags
    'train_mlp_variants': True,
    'train_selector_variants': True,
    'train_svms': True,
    'evaluate_feature_selection': True,
    'evaluate_reduced_feature_models': True,
}
EOL

# Step 3: Run the main pipeline with the temporary config
echo "Step 2: Running main pipeline with the following settings:"
echo "  - Using $(if [ "$USE_SMALL_SPLIT" = true ]; then echo "small"; else echo "standard"; fi) split"
echo "  - Max epochs: ${MAX_EPOCHS}"
echo "  - Resume training: ${RESUME_TRAINING}"

# Use the temporary config file
export PYTHONPATH=.:${PYTHONPATH}
ORIGINAL_CONFIG="src/config.py"
mv "${ORIGINAL_CONFIG}" "${ORIGINAL_CONFIG}.bak"
mv "${CONFIG_FILE}" "${ORIGINAL_CONFIG}"

# Run the main script
python src/main.py

# Restore the original config file
mv "${ORIGINAL_CONFIG}.bak" "${ORIGINAL_CONFIG}"

echo "Pipeline completed." 