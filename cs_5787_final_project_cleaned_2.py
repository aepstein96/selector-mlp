# -*- coding: utf-8 -*-
"""CS_5787_Final_Project Cleaned 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DTCyXffRC4tPxdlMNngPrBhgn77s5HJL
"""

import os
try:
  os.environ['COLAB_TPU_ADDR']
  print("TPU found! Installing cloud TPU client...")
  !pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl
except:
  print("No TPU found!")

!pip install scanpy
!pip install pytorch_lightning

import scanpy as sc
import pandas as pd
import os
import numpy as np
from torch import nn, optim
import torch
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import pytorch_lightning as pl
import torch.nn.functional as F
import torchmetrics
# from ray import tune
from google.colab import drive
import scipy.sparse
from pytorch_lightning.trainer.supporters import CombinedLoader

drive.mount('/content/gdrive')
main_dir = "/content/gdrive/My Drive/CS_5787/Final Project"
os.chdir(main_dir)

adata = sc.read_h5ad("data/adata_variable_small.h5ad")
adata_full = sc.read_h5ad("data/adata_variable.h5ad")

adata_full.obs

adata_full.obs['Main_cluster_name'].value_counts()

def createDataset(adata, y_column, sparse=True, sparse_type='CSR'):
  if sparse:
    if sparse_type == 'CSR':
      X = scipy.sparse.csr_matrix(adata.X)
      X = torch.sparse_csr_tensor(X.indptr, X.indices, X.data, X.shape, requires_grad=True)
    elif sparse_type == 'COO':
      X = scipy.sparse.coo_matrix(adata.X)
      indices = np.vstack((X.row, X.col))
      X = torch.sparse_coo_tensor(indices, X.data, X.shape, requires_grad=True)
    else:
      raise ValueError("Valid sparse_type is 'CSR' or 'COO'.")
  else:
    X = torch.from_numpy(adata.X.toarray())
  y_col = adata.obs[y_column]
  if y_col.dtype.name == 'category': # categorical data, e.g. clusters
    y = torch.from_numpy(y_col.cat.codes.values).long()
  else:
    y = torch.from_numpy(y_col.values).long()
  return TensorDataset(X, y)

class MultiClassifier(pl.LightningModule):

  def __init__(self, Net,  num_features, num_classes, lossFunc=nn.CrossEntropyLoss(), lr=0.001, alpha=0.01, noise_std=0, optimizer=optim.AdamW, betas=(0.9, 0.999), weight_decay=0.01, **net_args):
    super().__init__()
    self.save_hyperparameters(ignore=['Model', 'lossFunc', 'optimizer'])
    self.net = Net(num_features, num_classes, **net_args)
    self.optimizer = optimizer
    self.lossFunc = lossFunc
    self.accFunc = torchmetrics.Accuracy()

  def forward(self, data): # Final predictions, including softmax
    return torch.softmax(self.net(data), dim=1)

  def training_step(self, batch, batch_idx):
    X, y = batch
    y_pred = self.net(X + self.hparams.noise_std*torch.randn(X.shape, device=self.device))
    reg_term = self.net.regularize()
    loss_term = self.lossFunc(y_pred, y)
    loss = loss_term + self.hparams.alpha * reg_term # Custom regularization term from net
    self.log("train_batch_loss", loss, on_step=True, on_epoch=False)
    self.log('reg_term', reg_term, on_step=True, on_epoch=False)
    self.log("train_batch_loss_term", loss_term, on_step=True, on_epoch=False)
    return loss

  # Dataloader idx 0: validation. Dataloader idx 1: training (optional)
  def validation_step(self, batch, batch_idx, dataloader_idx=0):
    X, y = batch
    y_pred = self.net(X)
    loss = self.lossFunc(y_pred, y)
    acc = self.accFunc(torch.softmax(y_pred, dim=1), y)
    if dataloader_idx == 0:
      self.log('val_eval_loss', loss, on_step=False, on_epoch=True)
      self.log('val_accuracy', acc, on_step=False, on_epoch=True)
    elif dataloader_idx == 1:
      self.log('train_eval_loss', loss, on_step=False, on_epoch=True)
      self.log('train_accuracy', acc, on_step=False, on_epoch=True)

  def configure_optimizers(self):
    return self.optimizer(self.net.parameters(), lr=self.hparams.lr, betas=self.hparams.betas, weight_decay=self.hparams.weight_decay)

# Modified torch.nn.Linear, borrowed from PyTorch github (https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py)
# Has no bias, and weights are put through a sigmoid so the result is between 0 and 1
class Selector(nn.Module):
    def __init__(self, features, std=1, device=None, dtype=None):
        super().__init__()
        self.features = features
        self.weight = nn.Parameter(torch.empty(features))
        self.register_parameter('bias', None) # Not sure if this is necessary yet
        self.reset_parameters(std)

    def reset_parameters(self, std):
        if std is None:
            std=np.sqrt(self.features)
        nn.init.normal_(self.weight, mean=0, std=std)

    def forward(self, x):
        return torch.mul(x, self.weight) # Element-wise multiplication

class SelectorMLP(nn.Module):
    def __init__(self, num_features, num_classes, Selector=None,  batch_norm=True, reg_type='L1/L2', L1_size=512, L2_size=128, L3_size=64, leak_angle=0.2, dropout=0.3):
      super().__init__()

      if Selector is None:
        self.selector = None
      else:
        self.selector = Selector(num_features, std=1)

      self.reg_type = reg_type

      if batch_norm:
        self.batch1 = nn.BatchNorm1d(L1_size)
        self.batch2 = nn.BatchNorm1d(L2_size)
        self.batch3 = nn.BatchNorm1d(L3_size)
      else:
        self.batch1 = nn.Identity()
        self.batch2 = nn.Identity()
        self.batch3 = nn.Identity()

      self.h1 = nn.Linear(num_features, L1_size)
      self.h2 = nn.Linear(L1_size, L2_size)
      self.h3 = nn.Linear(L2_size, L3_size)

      self.out = nn.Linear(L3_size, num_classes)
      self.relu = nn.LeakyReLU(leak_angle)
      self.dropout = nn.Dropout(dropout)

    def forward(self, x):
      if self.selector is not None:
        x = self.selector(x)

      x = self.dropout(self.relu(self.batch1(self.h1(x))))
      x = self.dropout(self.relu(self.batch2(self.h2(x))))
      x = self.dropout(self.relu(self.batch3(self.h3(x))))
      return self.out(x)

    def regularize(self):
      if self.selector is None or self.reg_type is None:
        return 0
      elif self.reg_type == 'L1':
        L1 = torch.abs(self.selector.weight).sum()
        return L1
      elif self.reg_type == 'L2':
        L2 = torch.sqrt((self.selector.weight**2).sum())
        return L2
      elif self.reg_type == 'L1/L2':
        L1 = torch.abs(self.selector.weight).sum()
        L2 = torch.sqrt((self.selector.weight**2).sum())
        return L1/L2

adata_test

!nvidia-smi



# Balancing classes
def evenClusters(adata, col, max_train=5000, max_val=1000, max_test=1000):
  train_cells = []
  val_cells = []
  test_cells = []
  max_total_cells = max_train + max_val + max_test
  for cluster, num in adata_full.obs[col].value_counts().iteritems():
    cluster_cells = adata.obs.index[adata.obs[col]==cluster].values
    num_cells = cluster_cells.shape[0]
    cell_multiplier = min(1, num_cells/max_total_cells)
    train = int(max_train*cell_multiplier)
    val = int(max_val*cell_multiplier)
    test = int(max_test*cell_multiplier)

    train_cells.append(cluster_cells[:train])
    val_cells.append(cluster_cells[train:train+val])
    test_cells.append(cluster_cells[train+val:train+val+test])

  adata_train = adata[np.concatenate(train_cells), :]
  adata_val = adata[np.concatenate(val_cells), :]
  adata_test = adata[np.concatenate(test_cells), :]

  return adata_train, adata_val, adata_test

col = 'Main_cluster_name'
sparse=False
adata_train, adata_val, adata_test = evenClusters(adata_full, col)
train_set = createDataset(adata_train, col, sparse)
val_set = createDataset(adata_val, col, sparse)
test_set = createDataset(adata_test, col, sparse)

# Universal settings
batch_size=256
num_workers=4

num_features = adata_train.shape[1] # 2000 to start
num_classes = len(train_set.tensors[1].unique()) # 40
lr=0.0003

# Testing MLP conditions
save_dir = 'logs_MLP_conditions'
conditions = [{'name':'MLP_final', 'dropout':0.3,'batch_norm':True, 'noise':0.2},
              {'name':'MLP_no_dropout', 'dropout':0,'batch_norm':True, 'noise':0.2},
              {'name':'MLP_no_batchnorm', 'dropout':0.3,'batch_norm':False, 'noise':0.2},
              {'name':'MLP_no_noise', 'dropout':0.3,'batch_norm':True, 'noise':0},
              {'name':'MLP_nothing', 'dropout':0,'batch_norm':False, 'noise':0}
]

for cond in conditions:
  print("Testing condition %s" % cond['name'])
  train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)
  train_eval_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)
  val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)

  mlp_model = MultiClassifier(SelectorMLP, num_features, num_classes, lr=lr, batch_norm=cond['batch_norm'], noise_std=cond['noise'], Selector=None, weight_decay=0.3, dropout=cond['dropout'])
  logger1 = pl.loggers.TensorBoardLogger(save_dir=save_dir, name=cond['name'])
  logger2 = pl.loggers.CSVLogger(save_dir=save_dir, name=cond['name'])
  trainer = pl.Trainer(max_epochs=50, accelerator='gpu', logger=[logger1, logger2], default_root_dir=save_dir)
  trainer.fit(mlp_model, train_loader, [val_loader, train_eval_loader])

def getLogs(folder, find_version=True):

  if find_version:
    versions = [f for f in os.listdir(folder) if f.startswith('version')]
    versions.sort()
    folder = os.path.join(folder, versions[-1])

  print("Reading logs from %s..." % folder)

  logs = pd.read_csv(os.path.join(folder, "metrics.csv"))
  logs.columns = [col.split('/')[0] for col in logs.columns]

  logs_step = logs[logs['train_batch_loss'].notnull()].dropna(axis=1)
  logs_step.set_index('step', inplace=True)
  logs_epoch = logs[logs['val_eval_loss'].notnull()].dropna(axis=1)
  logs_epoch.set_index('epoch', inplace=True)

  return logs_step, logs_epoch

logs_step_MLP = {}
logs_epoch_MLP = {}
for cond in conditions:
  logs_step_MLP[cond['name']], logs_epoch_MLP[cond['name']] = getLogs(os.path.join("logs_MLP_conditions", cond['name']))

logs_epoch_MLP.val_accuracy

fig, ax = plt.subplots(figsize=(10, 6))
colormap = plt.cm.get_cmap('Set1')
for cond, color in zip(conditions, colormap.colors):
  name = cond['name']
  ax.plot(logs_epoch_MLP[name]['train_eval_loss'], color=color, label='Train loss: %s' % name)
  ax.plot(logs_epoch_MLP[name]['val_eval_loss'], '--', color=color, label='Validation loss: %s' % name)
ax.legend()
ax.set_ylim([0, 0.4])

fig, axs = plt.subplots(1, 2, figsize=(10, 6))
colors = plt.cm.get_cmap('Set1').colors
conds = ['MLP_nothing', 'MLP_no_batchnorm']
names = ['Weight decay only', 'Noise and dropout']
for cond, name, color in zip(conds, names, colors):
  axs[0].plot(logs_epoch[cond]['train_eval_loss'], '--', color=color, label='%s: training loss' % name)
  axs[0].plot(logs_epoch[cond]['val_eval_loss'], color=color, label='%s: validation loss' % name)
  axs[1].plot(logs_epoch[cond]['train_accuracy'], '--', color=color, label='%s: training accuracy' % name)
  axs[1].plot(logs_epoch[cond]['val_accuracy'], color=color, label='%s: validation accuracy' % name)

for ax in axs:
  ax.legend()
  ax.set_xlabel('Epoch')
axs[0].set_ylabel('Cross-entropy loss')
axs[0].set_title('Training and validation loss')
axs[1].set_ylabel('Accuracy')
axs[1].set_title('Training and validation accuracy')
fig.savefig('figures/MLPLoss.png')

# Training various versions of selector
# Testing selector conditions
save_dir = 'logs_selector_bigger_alpha'
conditions = [{'name':'None', 'reg_type':None},
              {'name':'L1', 'reg_type':'L1'},
              {'name':'L2', 'reg_type':'L2'},
              {'name':'L1_L2', 'reg_type':'L1/L2'},
]
selector_models3 = {}
dropout=0.3
batch_norm=False
noise=0.2
alpha=0.05
for cond in conditions:
  print("Testing condition %s" % cond['name'])
  train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)
  train_eval_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)
  val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)

  selector_models3[cond['name']] = MultiClassifier(SelectorMLP, num_features, num_classes, lr=lr, alpha=alpha, batch_norm=batch_norm, noise_std=noise, Selector=Selector, reg_type=cond['reg_type'], weight_decay=0.3, dropout=dropout)
  logger1 = pl.loggers.TensorBoardLogger(save_dir=save_dir, name=cond['name'])
  logger2 = pl.loggers.CSVLogger(save_dir=save_dir, name=cond['name'])
  trainer = pl.Trainer(max_epochs=50, accelerator='gpu', logger=[logger1, logger2], default_root_dir=save_dir)
  trainer.fit(selector_models3[cond['name']], train_loader, val_loader)

for name, model in selector_models3.items():
  torch.save(model.net, os.path.join(save_dir, name+'.pkl'))

# Loading logs
save_dir = "logs_selector_conditions"
logs_step = {}
logs_epoch = {}
conditions = ['None', 'L1', 'L2', 'Selector_clamped']
for cond in conditions:
  logs_step[cond], logs_epoch[cond] = getLogs(os.path.join(save_dir, cond))

logs_step_MLP, logs_epoch_MLP = getLogs("logs_MLP_conditions/MLP_no_batchnorm")

# Retraining best MLP
save_dir = 'logs_MLP_conditions'
name = 'MLP_best'
selector_models = {}
dropout=0.3
batch_norm=False
noise=0.2

train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)
train_eval_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)
val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)

MLP_best = MultiClassifier(SelectorMLP, num_features, num_classes, lr=lr, batch_norm=batch_norm, noise_std=noise, Selector=None, weight_decay=0.3, dropout=dropout)
logger1 = pl.loggers.TensorBoardLogger(save_dir=save_dir, name=name)
logger2 = pl.loggers.CSVLogger(save_dir=save_dir, name=name)
trainer = pl.Trainer(max_epochs=50, accelerator='gpu', logger=[logger1, logger2], default_root_dir=save_dir)
trainer.fit(MLP_best, train_loader, val_loader)

class ClampedSelector(nn.Module):
    def __init__(self, features: int, std=1, device=None, dtype=None):

        #factory_kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        self.features = features
        #self.weight = nn.Parameter(torch.empty(features), **factory_kwargs)
        self.weight = nn.Parameter(torch.empty(features))
        self.register_parameter('bias', None) # Not sure if this is necessary yet
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.normal_(self.weight, mean=0, std=0.2)
        self.weight.data = torch.abs(self.weight.data)
    def forward(self, x):
        self.weight.data = torch.clamp(self.weight.data, min=0, max=1) # Result will be between 0 and 1
        return torch.mul(x, self.weight) # Element-wise multiplication

# Training clamped selector
save_dir = 'logs_selector_conditions'
name = 'Selector_clamped'
num_features = 2000
selector_models = {}
dropout=0.3
batch_norm=False
noise=0.2

train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=True)
train_eval_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)
val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=num_workers, shuffle=False)

selector_clamped = MultiClassifier(SelectorMLP, num_features, num_classes, lr=lr, batch_norm=batch_norm, noise_std=noise, reg_type='L1', Selector=ClampedSelector, weight_decay=0.3, dropout=dropout)
logger1 = pl.loggers.TensorBoardLogger(save_dir=save_dir, name=name)
logger2 = pl.loggers.CSVLogger(save_dir=save_dir, name=name)
trainer = pl.Trainer(max_epochs=50, accelerator='gpu', logger=[logger1, logger2], default_root_dir=save_dir)
trainer.fit(selector_clamped, train_loader, val_loader)

torch.save(selector_clamped.net, "logs_selector_conditions/selector_clamped.pkl")

torch.save(MLP_best.net,"logs_MLP_conditions/MLP_best.pkl")

# Creating figure
logs_epoch['MLP'] = logs_epoch_MLP
fig, axs = plt.subplots(1, 2, figsize=(10, 6))
colors = plt.cm.get_cmap('Set1').colors
conds = logs_epoch.keys()
names = ['Selector: No regularization', 'Selector: L1', 'Selector: L2', 'Selector: Clamped L1', 'MLP']
for cond, name, color in zip(conds, names, colors):
  #axs[0].plot(logs_epoch[cond]['train_eval_loss'], '--', color=color, label='%s: training loss' % name)
  axs[0].plot(logs_epoch[cond]['val_eval_loss'], color=color, label=name)
  #axs[1].plot(logs_epoch[cond]['train_accuracy'], '--', color=color, label='%s: training accuracy' % name)
  axs[1].plot(logs_epoch[cond]['val_accuracy'], color=color, label=name)

for ax in axs:
  ax.legend()
  ax.set_xlabel('Epoch')
axs[0].set_ylabel('Cross-entropy loss')
axs[0].set_title('Validation loss')
axs[1].set_ylabel('Accuracy')
axs[1].set_title('Validation accuracy')
fig.savefig('figures/SelectorLossWithClamping.png')

def selectFeatures(feat_nums, indices, net, X, y, lossFunc=nn.CrossEntropyLoss(), accFunc=torchmetrics.Accuracy()):
  with torch.no_grad():
    losses = []
    accs = []
    filter = np.zeros(indices.shape[0])
    for num in tqdm(feat_nums):
      filter[indices[:num]] = 1
      X_filtered = torch.mul(X, torch.from_numpy(filter).float())
      y_pred = net(X_filtered)
      losses.append(lossFunc(y_pred, y))
      accs.append(accFunc(y_pred, y))

    losses = torch.stack(losses).cpu().detach().numpy()
    accs = torch.stack(accs).cpu().detach().numpy()
    return losses, accs

def testFeatureSelection(net, X, y, group_size=50, random_reps=5, **kwargs):
  with torch.no_grad():
    if net.selector is None: # For an MLP, sum up all of the weights corresponding to each feature
      weight_abs = np.sum(np.abs(net.h1.weight.cpu().detach().numpy()), axis=0)
    else:
      weight_abs = np.abs(net.selector.weight.cpu().detach().numpy())

    feat_nums = np.arange(0, weight_abs.shape[0], group_size)

    # Get sorted weights
    idx_sorted = np.argsort(weight_abs)[::-1]
    losses_sorted, accs_sorted = selectFeatures(feat_nums, idx_sorted, net, X, y, **kwargs)

    # Get random weight results
    losses_random = np.zeros(feat_nums.shape[0])
    accs_random = np.zeros(feat_nums.shape[0])

    for i in range(random_reps):
      idx_random = np.random.permutation(weight_abs.shape[0])
      losses_tmp, accs_tmp = selectFeatures(feat_nums, idx_random, net, X, y, **kwargs)
      losses_random += losses_tmp
      accs_random += accs_tmp

    losses_random /= random_reps
    accs_random /= random_reps
    return losses_sorted, accs_sorted, losses_random, accs_random, feat_nums

MLP_best.net.h1.weight.sum(axis=0).shape

# Selectors and MLP: eliminating dimensions
selector_models = {}
selector_models['No regularization'] = torch.load('logs_selector_conditions/None.pkl')
selector_models['L1'] = torch.load('logs_selector_conditions/L1.pkl')
selector_models['L2'] = torch.load('logs_selector_conditions/L2.pkl')
selector_models['Clamped'] = torch.load('logs_selector_conditions/selector_clamped.pkl')
selector_models['MLP'] = torch.load("logs_MLP_conditions/MLP_best.pkl")

# Loading other models

selector_models2

losses_sorted = {}
accs_sorted = {}
losses_random = {}
accs_random = {}
for name, model in selector_models.items():
  print("Testing %s" % name)
  losses_sorted[name], accs_sorted[name], losses_random[name], accs_random[name], feat_nums = testFeatureSelection(model, X_val, y_val)

for name in accs_sorted2.keys():
  plt.plot(accs_sorted2[name], label=name)
plt.legend()



# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./logs_MLP_conditions/MLP_200

# Testing linear SVM
from sklearn.svm import LinearSVC

X_train, y_train = [tensor.numpy() for tensor in train_set.tensors]
l2_model = LinearSVC(penalty='l2', loss='squared_hinge', dual=False)
l1_model = LinearSVC(penalty='l1', loss='squared_hinge', dual=False)

l1_model.fit(X_train, y_train)
l2_model.fit(X_train, y_train)

X_val, y_val = [tensor.numpy() for tensor in val_set.tensors]

y_pred_l1 = l1_model.predict(X_val)
y_pred_l2 = l2_model.predict(X_val)

y_pred_l1

(y_pred_l1==y_val).sum()/len(y_pred_l1)

(y_pred_l2==y_val).sum()/len(y_val)

X_val, y_val = val_set.tensors
y_val_numpy = y_val.cpu().detach().numpy()

from sklearn.metrics import log_loss
def selectFeaturesSVM(feat_nums, indices, model, X, y):
    accs = []
    filter = np.zeros(indices.shape[0])
    for num in tqdm(feat_nums):
      filter[indices[:num]] = 1
      X_filtered = np.multiply(X, filter)
      y_pred = model.predict(X_filtered)
      acc = (y_pred==y).sum() / len(y)
      accs.append(acc)

    accs = np.array(accs, dtype=float)
    return accs

def testFeatureSelectionSVM(model, X, y, group_size=50, random_reps=5, **kwargs):

    if type(X) == torch.Tensor:
      X = X.cpu().detach().numpy()
    if type(y) == torch.Tensor:
      y = y.cpu().detach().numpy()

    weight_abs = np.abs(model.coef_).sum(axis=0) # hope the sum makes sense
    feat_nums = np.arange(0, weight_abs.shape[0], group_size)

    # Get sorted weights
    idx_sorted = np.argsort(weight_abs)[::-1]
    accs_sorted = selectFeaturesSVM(feat_nums, idx_sorted, model, X, y, **kwargs)

    # Get random weight results
    accs_list = []
    accs_random = np.zeros(feat_nums.shape[0])
    for i in range(random_reps):
      idx_random = np.random.permutation(weight_abs.shape[0])
      accs_tmp = selectFeaturesSVM(feat_nums, idx_random, model, X, y, **kwargs)
      accs_random += accs_tmp

    accs_random /= random_reps
    return accs_sorted, accs_random, feat_nums

type(X_val)

import pickle
SVM_models = {}
with open("SVMs/l1_model.pkl", 'rb') as f:
  SVM_models['SVM_L1'] = pickle.load(f)
with open("SVMs/l2_model.pkl", 'rb') as f:
  SVM_models['SVM_L2'] = pickle.load(f)
MLP_models = {}
MLP_models['Selector_None'] = torch.load("logs_selector_conditions/None.pkl")
MLP_models['Selector_L1'] = torch.load("logs_selector_conditions/L1.pkl")
MLP_models['Selector_L2'] = torch.load("logs_selector_conditions/L2.pkl")
MLP_models['Selector_Clamped_L1'] = torch.load("logs_selector_conditions/selector_clamped.pkl")
MLP_models['MLP'] = torch.load('logs_MLP_conditions/MLP_best.pkl')

losses_sorted = {}
accs_sorted = {}
losses_random = {}
accs_random = {}
for name, model in SVM_models.items():
  accs_sorted[name], accs_random[name], feat_nums = testFeatureSelectionSVM(model, X_val, y_val)
for name, model in MLP_models.items():
  losses_sorted[name], accs_sorted[name], losses_random[name], accs_random[name], feat_nums = testFeatureSelection(model, X_val, y_val)

X_val, y_val = val_set.tensors
_, accs_sorted['Selector_Clamped_L1'], _, _, feat_nums = testFeatureSelection(MLP_models['Selector_Clamped_L1'], X_val, y_val, random_reps=0)

with open("feature_selection/losses_sorted.pkl", 'wb') as f:
  pickle.dump(losses_sorted, f)
with open("feature_selection/losses_random.pkl", 'wb') as f:
  pickle.dump(losses_random, f)
with open("feature_selection/accs_sorted.pkl", 'wb') as f:
  pickle.dump(accs_sorted, f)
with open("feature_selection/accs_random.pkl", 'wb') as f:
  pickle.dump(accs_random, f)

with open("feature_selection/losses_sorted.pkl", 'rb') as f:
  losses_sorted = pickle.load(f)
with open("feature_selection/losses_random.pkl", 'rb') as f:
  losses_random = pickle.load(f)
with open("feature_selection/accs_sorted.pkl", 'rb') as f:
  accs_sorted = pickle.load(f)
with open("feature_selection/accs_random.pkl", 'rb') as f:
  accs_random = pickle.load(f)

fig, ax = plt.subplots(figsize=(10, 6))
labels = ['SVM: L1 regularization', 'SVM: L2 regularization', 'Selector: No regularization', 'Selector: L1 regularization', 'Selector: L2 regularization', 'Selector: L1, clamped', 'MLP']
for key, label in zip(accs_sorted.keys(), labels):
  ax.plot(feat_nums, accs_sorted[key], label=label)
ax.plot(feat_nums, accs_random['Selector_L1'], label='Selector: L1 regularization, random order')
ax.legend()
ax.set_xlabel('Number of features retained')
ax.set_ylabel('Validation accuracy')
fig.savefig('figures/feature_selection_with_clamped.png')

feat_nums[accs_sorted['Selector_L1'] > 0.93][0]

import pickle
with open("SVMs/l1_model.pkl", "rb") as f:
  L1_SVM = pickle.load(f)
L1_Selector = torch.load("logs_selector_conditions/L1.pkl")
MLP = torch.load('logs_MLP_conditions/MLP_best.pkl')

import sklearn

def chooseFeatures(features, *datasets):
  out_list = []
  for dataset in datasets:
    X, y = dataset.tensors
    out_list.append(TensorDataset(X[:, features.copy()], y))
  return out_list

MLP.h1.weight.shape

num_features=100
L1_selector_weights = torch.abs(L1_Selector.selector.weight).cpu().detach().numpy()
L1_selector_top_features = np.argsort(L1_selector_weights)[::-1][:num_features]
train_set_selector, val_set_selector, test_set_selector = chooseFeatures(L1_selector_top_features, train_set, test_set, val_set)

L1_SVM_weights = np.abs(L1_SVM.coef_).sum(axis=0)
L1_SVM_top_features = np.argsort(L1_SVM_weights)[::-1][:num_features]
train_set_SVM, val_set_SVM, test_set_SVM = chooseFeatures(L1_SVM_top_features, train_set, test_set, val_set)

MLP_weights = torch.abs(MLP.h1.weight).sum(axis=0).cpu().detach().numpy()
MLP_top_features = np.argsort(MLP_weights)[::-1][:num_features]
train_set_MLP, val_set_MLP, test_set_MLP = chooseFeatures(MLP_top_features, train_set, test_set, val_set)

MLP

# Retraining SVM
from sklearn.svm import LinearSVC
L1_SVM100 = LinearSVC(penalty='l1', dual=False)
X_train, y_train = train_set_SVM.tensors
L1_SVM100.fit(X_train.cpu().detach().numpy(), y_train.cpu().detach().numpy())

# Retraining selector with 200 features
save_dir = 'logs_selector_conditions'
name='L1_100'
num_features = train_set_selector.tensors[0].shape[1]
dropout=0.3
batch_norm=False
noise=0.2

train_loader = DataLoader(train_set_selector, batch_size=batch_size, num_workers=num_workers, shuffle=True)
#train_eval_loader = DataLoader(train_set_selector, batch_size=batch_size, num_workers=num_workers, shuffle=False)
val_loader = DataLoader(val_set_selector, batch_size=batch_size, num_workers=num_workers, shuffle=False)

L1_Selector100 = MultiClassifier(SelectorMLP, num_features, num_classes, lr=lr, batch_norm=batch_norm, noise_std=noise, reg_type='L1', Selector=Selector, weight_decay=0.3, dropout=dropout)
logger1 = pl.loggers.TensorBoardLogger(save_dir=save_dir, name=name)
logger2 = pl.loggers.CSVLogger(save_dir=save_dir, name=name)
trainer = pl.Trainer(max_epochs=50, accelerator='gpu', logger=[logger1, logger2], default_root_dir=save_dir)
trainer.fit(L1_Selector100, train_loader, val_loader)
torch.save(L1_Selector100.net, os.path.join(save_dir,"L1_100.pkl"))

# Retraining MLP with L1 features
save_dir = 'logs_MLP_conditions'
name='MLP_100_L1Features'
num_features = train_set_selector.tensors[0].shape[1]
dropout=0.3
batch_norm=False
noise=0.2

train_loader = DataLoader(train_set_selector, batch_size=batch_size, num_workers=num_workers, shuffle=True)
#train_eval_loader = DataLoader(train_set_selector, batch_size=batch_size, num_workers=num_workers, shuffle=False)
val_loader = DataLoader(val_set_selector, batch_size=batch_size, num_workers=num_workers, shuffle=False)

MLP100 = MultiClassifier(SelectorMLP, num_features, num_classes, lr=lr, batch_norm=batch_norm, noise_std=noise, Selector=None, weight_decay=0.3, dropout=dropout)
logger1 = pl.loggers.TensorBoardLogger(save_dir=save_dir, name=name)
logger2 = pl.loggers.CSVLogger(save_dir=save_dir, name=name)
trainer = pl.Trainer(max_epochs=50, accelerator='gpu', logger=[logger1, logger2], default_root_dir=save_dir)
trainer.fit(MLP100, train_loader, val_loader)
torch.save(MLP100.net, os.path.join(save_dir,"MLP_100Features.pkl"))

# Retraining MLP with 100 features
save_dir = 'logs_MLP_conditions'
name='MLP_100'
num_features = train_set_selector.tensors[0].shape[1]
dropout=0.3
batch_norm=False
noise=0.2

train_loader = DataLoader(train_set_MLP, batch_size=batch_size, num_workers=num_workers, shuffle=True)
#train_eval_loader = DataLoader(train_set_selector, batch_size=batch_size, num_workers=num_workers, shuffle=False)
val_loader = DataLoader(val_set_MLP, batch_size=batch_size, num_workers=num_workers, shuffle=False)

MLP100 = MultiClassifier(SelectorMLP, num_features, num_classes, lr=lr, batch_norm=batch_norm, noise_std=noise, Selector=None, weight_decay=0.3, dropout=dropout)
logger1 = pl.loggers.TensorBoardLogger(save_dir=save_dir, name=name)
logger2 = pl.loggers.CSVLogger(save_dir=save_dir, name=name)
trainer = pl.Trainer(max_epochs=50, accelerator='gpu', logger=[logger1, logger2], default_root_dir=save_dir)
trainer.fit(MLP100, train_loader, val_loader)
torch.save(MLP100.net, os.path.join(save_dir,"MLP_100.pkl"))

val_set_selector

accs_sorted_SVM200, _, feat_nums = testFeatureSelectionSVM(L1_SVM200, *val_set_SVM.tensors, group_size=5, random_reps=0)
_, accs_sorted_Selector200, _, _, feat_nums = testFeatureSelection(L1_Selector200.net, *val_set_selector.tensors, group_size=5, random_reps=0)
_, accs_sorted_MLP200, _, _, feat_nums = testFeatureSelection(MLP200.net, *val_set_MLP.tensors, group_size=5, random_reps=0)

accs_sorted_SVM100, _, feat_nums = testFeatureSelectionSVM(L1_SVM100, *val_set_SVM.tensors, group_size=5, random_reps=0)
_, accs_sorted_Selector100, _, _, feat_nums = testFeatureSelection(L1_Selector100.net, *val_set_selector.tensors, group_size=5, random_reps=0)
_, accs_sorted_MLP100, _, _, feat_nums = testFeatureSelection(MLP100.net, *val_set_MLP.tensors, group_size=5, random_reps=0)

print(accs_sorted_Selector100[-1])
print(accs_sorted_MLP100[-1])

feat_nums_2000 = np.arange(0, 2000, 50)
feat_nums_200 = np.arange(0, 200, 5)
fig, axs = plt.subplots(2, 1, figsize=(10, 8))
labels = ['SVM: L1 regularization', 'SVM: L2 regularization', 'Selector: No regularization', 'Selector: L1 regularization', 'Selector: L2 regularization', 'Selector: L1, clamped', 'MLP']
for key, label in zip(accs_sorted.keys(), labels):
  axs[0].plot(feat_nums_2000, accs_sorted[key], label=label)
axs[0].plot(feat_nums_2000, accs_random['Selector_L1'], label='Selector: L1 regularization, random order')
axs[0].legend()
axs[0].set_xlabel('Number of features retained')
axs[0].set_ylabel('Validation accuracy')
axs[0].set_title('Trained on all 2000 features')
axs[0].set_ylim(0, 1)
axs[0].set_xlim(0, 2000)
axs[1].plot(feat_nums_200, accs_sorted_SVM200, color='tab:blue', label='SVM: L1 regularization')
axs[1].plot(feat_nums_200, accs_sorted_Selector200, color='tab:red', label='SelectorMLP: L1 regularization')
axs[1].plot(feat_nums_200, accs_sorted_MLP200, color='tab:pink', label='MLP')
axs[1].set_xlabel('Number of features retained')
axs[1].set_ylabel('Validation accuracy')
axs[1].set_title('Trained on top 200 features')
axs[1].legend()
axs[1].set_ylim(0, 1)
axs[1].set_xlim(0, 200)
fig.tight_layout()
fig.savefig('figures/feature_selection_2000_200.png')

def selectFeatures(feat_nums, indices, net, X, y, lossFunc=nn.CrossEntropyLoss(), accFunc=torchmetrics.Accuracy()):
  with torch.no_grad():
    losses = []
    accs = []
    filter = np.zeros(indices.shape[0])
    for num in tqdm(feat_nums):
      filter[indices[:num]] = 1
      X_filtered = torch.mul(X, torch.from_numpy(filter).float())
      y_pred = net(X_filtered)
      losses.append(lossFunc(y_pred, y))
      accs.append(accFunc(y_pred, y))

    losses = torch.stack(losses).cpu().detach().numpy()
    accs = torch.stack(accs).cpu().detach().numpy()
    return losses, accs

from sklearn.metrics import confusion_matrix
import sklearn

L1_selector_weights = torch.abs(L1_Selector.selector.weight).cpu().detach().numpy()
L1_selector_top_features = np.argsort(L1_selector_weights)[::-1][:num_features]
train_set_selector, val_set_selector, test_set_selector = chooseFeatures(L1_selector_top_features, train_set, test_set, val_set)

L1_SVM_weights = np.abs(L1_SVM.coef_).sum(axis=0)
L1_SVM_top_features = np.argsort(L1_SVM_weights)[::-1][:num_features]
train_set_SVM, val_set_SVM, test_set_SVM = chooseFeatures(L1_SVM_top_features, train_set, test_set, val_set)

MLP_weights = torch.abs(MLP.h1.weight).sum(axis=0).cpu().detach().numpy()
MLP_top_features = np.argsort(MLP_weights)[::-1][:num_features]
train_set_MLP, val_set_MLP, test_set_MLP = chooseFeatures(MLP_top_features, train_set, test_set, val_set)

# Calculating final test accuracies (per-class) for several models
# MLP (2000 features), L1 selector (2000 features), L1 selector (200 features), L1 SVM (2000 features), L1 SVM (200 features)
def getTestAccuracy(model, dataset, features=None):
  X, y = dataset.tensors
  if features is not None:
    filter = torch.zeros(X.shape[1])
    filter[features.copy()] = 1
    X = torch.mul(X, filter)

  y = y.cpu().detach().numpy()

  if type(model) == sklearn.svm._classes.LinearSVC:
    X = X.cpu().detach().numpy()
    y_pred = model.predict(X)
  else:
    with torch.no_grad():
      model.eval()
      y_pred = torch.softmax(model(X), dim=1).cpu().detach().numpy().argmax(axis=1)
  accuracy = (y==y_pred).sum()/y.shape[0]
  per_class_accuracy = confusion_matrix(y_pred, y, normalize='true').diagonal()
  return accuracy, per_class_accuracy.mean()

with open("SVMs/l1_model.pkl", "rb") as f:
  L1_SVM = pickle.load(f)
L1_Selector = torch.load("logs_selector_conditions/L1.pkl")
MLP = torch.load('logs_MLP_conditions/MLP_best.pkl')

model_types = ['MLP','L1_Selector','L1_SVM']
all_models = {}
all_models['MLP'] = {'model_2000':MLP,'model_100':MLP100,'dataset_100':test_set_MLP,'features_100':MLP_top_features}
all_models['L1_Selector'] = {'model_2000':L1_Selector,'model_100':L1_Selector100,'dataset_100':test_set_selector,'features_100':L1_selector_top_features}
all_models['L1_SVM'] = {'model_2000':L1_SVM,'model_100':L1_SVM100,'dataset_100':test_set_SVM,'features_100':L1_SVM_top_features}

#models_2000 = {'MLP':MLP,'L1_Selector':L1_Selector,'L1_SVM':L1_SVM}
#models_200 = {'MLP':MLP200,'L1_Selector':L1_Selector200,'L1_SVM':L1_SVM200}
#datasets_200 = {'MLP':test_set_MLP, 'L1_Selector':test_set_selector, 'L1_SVM':test_set_SVM}
#features_200 = {'MLP':MLP_top_features, 'L1_Selector':L1_selector_top_features, 'L1_SVM':L1_SVM_top_features}

mean_per_class_accuracies = pd.DataFrame(index=['2000 features','100 features (before re-training)', '100 features (re-trained)'])
for model, info in all_models.items():
  _, acc_2000 = getTestAccuracy(info['model_2000'], test_set)
  _, acc_100before = getTestAccuracy(info['model_2000'], test_set, features=info['features_100'])
  _, acc_100after = getTestAccuracy(info['model_100'], info['dataset_100'])
  mean_per_class_accuracies[model] = [acc_2000, acc_100before, acc_100after]

mean_per_class_accuracies_1.to_pickle('per_class_accuracy_100.pkl')

import pickle
import numpy as np
import matplotlib.pyplot as plt
with open('per_class_accuracy_100.pkl', 'rb') as f:
  mean_per_class_accuracies_100 = pickle.load(f)
with open('per_class_accuracy.pkl', 'rb') as f:
  mean_per_class_accuracies_200 = pickle.load(f)

mean_per_class_accuracies_100.iloc[1:, :]

mean_per_class_accuracies = pd.concat([mean_per_class_accuracies_200, mean_per_class_accuracies_100.iloc[1:, :]])

mean_per_class_accuracies

fig, ax = plt.subplots(figsize=(10, 6))
mean_per_class_accuracies.T.plot.bar(ax=ax)
labels = ['MLP', 'L1 Selector', 'L1 SVM']
ax.set_xticklabels(labels, rotation=0)
ax.set_ylabel('Mean test per-class accuracy')
fig.savefig('figures/test_accuracy_all.png')

with open("model_data.pkl", "wb") as f:
  pickle.dump(all_models, f)

np.in1d(L1_selector_top_features, MLP_top_features).sum()

np.in1d(L1_selector_top_features, L1_SVM_top_features).sum()

accs_sorted_L1, accs_random_L1, feat_nums = testFeatureSelectionSVM(l1_model, X_val, y_val, group_size=50, random_reps=)

accs_sorted_L2, accs_random_L2, feat_nums = testFeatureSelectionSVM(l2_model, X_val, y_val, group_size=50)

fig, ax = plt.subplots(figsize=(10, 6))
names = ['None', 'L1', 'L2', 'MLP']
labels = ['Selector: no regularization', 'Selector: L1 regularization', 'Selector: L2 regularization', 'MLP']
for name, label in zip(names, labels):
  ax.plot(feat_nums, accs_sorted2[name], label=label)

ax.plot(feat_nums, accs_sorted_L1, label='SVM: L1 normalization')
ax.plot(feat_nums, accs_sorted_L2, label='SVM: L2 normalization')
ax.plot()
ax.legend()
ax.set_xlabel('Number of features used')
ax.set_ylabel('Validation accuracy')
fig.savefig('figures/feature_selection.png')

import pickle
with open("SVMs/l1_model.pkl", 'wb') as f:
  pickle.dump(l1_model, f)
with open("SVMs/l2_model.pkl", 'wb') as f:
  pickle.dump(l2_model, f)

with open("SVMs/l1_model.pkl", 'rb') as f:
  test = pickle.load(f)

test.predict(X_val)

"""# IDEAS FOR REGULARIZATION
* Batch norm
* Modify loss function (L1)?
  * Should L1 apply for all weights?
"""

import matplotlib.pyplot as plt
plt.plot(mlp_model.train_loss_list)



test_f(train_set)

crow_indices = test.indptr
col_indices = test.indices
values = test.data
size = test.shape

tensor = torch.sparse_csr_tensor(crow_indices, col_indices, values, size)

tensor.sum()

adata.X.sum()



